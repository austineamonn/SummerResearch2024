{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin we have some standard python libraries to import that we will use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no existing dataset that contained the data needed for this project. Thus first we must generate a synthetic dataset. The dataset will be generated based on a variety of real data, mappings between datasets, and artificially generated lists. \n",
    "\n",
    "First we import the Data class which contains all the data needed to generate the synthetic dataset.\n",
    "\n",
    "Next we import the DataGenerator class for the CPU. Note that a version does exist that runs on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datafiles_for_data_construction.data import Data\n",
    "from data_generation.data_generation_CPU import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the Data and DataGenerator classes. The Data class allows us to access all the data needed to generate the synthetic dataset and the DataGenerator class allows us to use the functions needed to generate the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data_generator = DataGenerator(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the data look like? Some of the data is a list of values. Some lists were generated synthetically, others were pulled from various sources. More information can be found in the README file. Here is a list of learning styles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.learning_style()[\"learning_style_list\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the data is a dictionary. Some dictionaries map different lists together while others map lists to demographic statistics on how common each item is. This dictionary maps the learning styles to the percentage of people that have said style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.learning_style()[\"learning_style\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the generate_synthetic_dataset function to create a dataset from all the data. This function has two inputs:\n",
    "- number of samples (an integer) which tell the function how many 'students' we want in our dataset\n",
    "- batch size (an integer) which tells the function how to split up the work to prevent overloading the computer.\n",
    "You can change the values if you want to generate more or less data. Be careful as higher values for number of samples will lead to a longer runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100 # You can change these values if you want\n",
    "batch_size = 10 # Batch size should be about 1/10 of the number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the function. Use the time library to see how long the generator takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "synthetic_data = data_generator.generate_synthetic_dataset(num_samples, batch_size)\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'generate_synthetic_dataset' outputs a pandas dataframe. Lets look at the top 5 elements of the dataframe. You can look back at the README file to get a better sense of what each column contains and how it was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data.head(n=5) # Change n to larger numbers to see more rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have columns that are lists and columns that are strings. Machine learning models need the input data to be numerical. Thus some data preprocessing is required.\n",
    "\n",
    "We import the Preprocessing class to do the preprocessing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocessing.preprocessing import PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the Preprocessing class there are two functions that do the main preprocessing work:\n",
    "- 'stringlist_to_binarylist': converts lists of strings into a binary list\n",
    "- 'string_list_to_numberedlist': converts lits of strings into a numbered list.\n",
    "\n",
    "Imagine the full options available are ['alice', 'bob', 'charlie']\n",
    "Thus for the entry ['alice', 'charlie'] we get:\n",
    "[1,0,1] for 'stringlist_to_binarylist'\n",
    "[0,2] for 'string_list_to_numberedlist'\n",
    "\n",
    "When we instantiate the class and call the 'preprocess_dataset' function both of the above functions will be called on certain columns. 'stringlist_to_binarylist' is called on 'learning styles' and 'string_list_to_numberedlist' is called on all the other lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PreProcessing(data)\n",
    "start_time = time.time()\n",
    "preprocessed_data = preprocessor.preprocess_dataset(synthetic_data)\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'preprocess_dataset' outputs a pandas dataframe. Lets look at the top 5 elements of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.head(n=5) # Change n to larger numbers to see more rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been preprocessed we must privatize the data to keep it safe.\n",
    "\n",
    "We import the Privatizer class to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_privatization.privatization import Privatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of privatization methods you can try:\n",
    "- Basic Differential Privacy (laplace noise addition)\n",
    "- Uniform Noise Differential Privacy (uniform noise addition)\n",
    "- Shuffling\n",
    "Both Differential Privacy types can be done with or without list lengthening. This means the list columns like 'previous courses' could be lengthened according to the noise addition function. Let's try basic differential privacy with list lengthening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privatizer = Privatizer(data, style='basic differential privacy', list_length=True)\n",
    "# Other 'style' options: 'uniform', 'shuffle', 'full shuffle' (full shuffle shuffles all of the rows)\n",
    "# Can set 'list_length' to false if you don't want to allow the list sizes to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call 'privatize_dataset'. Use the time library to see how long the privatizer takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "privatized_data = privatizer.privatize_dataset(preprocessed_data)\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'preprocess_dataset' outputs a pandas dataframe. Lets look at the top 5 elements of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privatized_data.head(n=5) # Change n to larger numbers to see more rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have the problem of long lists. The 'previous courses list' can be over 30 elements long! Thus we call a new function from the Preprocessor class, 'create_RNN_models'. Three different recurrent neural network models are used to reduce the dimension of each list to 1 element. The three networks are: Simple, GRU (Gated Recurrent Units), and LSTM (Long Term Short Memory).\n",
    "\n",
    "Since 'create_RNN_models' takes in a dataframe, there is no need to create a new instance of the Preprocessor class. Thus we should call:\n",
    "- 'privatized_data': reduce dimensionality\n",
    "- 'preprocessed_data': give a null for comparison at the end\n",
    "- 'preprocessed_data' with 'utility=True': reduce dimensionality of the utility columns\n",
    "\n",
    "Let's also calculate and compare the runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "privatized_data_reduced = preprocessor.create_RNN_models(privatized_data)\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(f'Privatized data runtime: {runtime}')\n",
    "\n",
    "start_time = time.time()\n",
    "nonprivatized_data_reduced = preprocessor.create_RNN_models(preprocessed_data)\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(f'Nonprivatized data runtime: {runtime}')\n",
    "\n",
    "start_time = time.time()\n",
    "utility_cols_reduced = preprocessor.create_RNN_models(preprocessed_data, utility=True)\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(f'Utility columns runtime: {runtime}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'create_RNN_models' outputs a pandas dataframe. Lets look at the top 5 elements for each of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(privatized_data_reduced.head(n=5))\n",
    "print(nonprivatized_data_reduced.head(n=5))\n",
    "print(utility_cols_reduced.head(n=5))\n",
    "# Change n to larger numbers to see more rows of the dataframe"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
