@article{alkooheji2018,
  title = {Learning {{Style Preferences}} among {{College Students}}},
  author = {Alkooheji, Lamya and {Al-Hattami}, Abdulghani},
  year = {2018},
  month = sep,
  journal = {International Education Studies},
  volume = {11},
  number = {10},
  pages = {50},
  issn = {1913-9039, 1913-9020},
  doi = {10.5539/ies.v11n10p50},
  urldate = {2024-12-03},
  abstract = {The purpose of this study was to determine what factors other than individual preferences affect undergraduate students' learning style preferences, if learning style is influenced by gender, age, college affiliation and/or type of activities. A total of 185 students from the University of Bahrain, Bahrain, participated in an online VARK (Visual, Aural, Read/Write and Kinesthetic) for younger people questionnaire. The questionnaire consisted of 16 items about learning style preferences and three about participants' demographics. The results showed that participants generally preferred multi-modular learning style with both kinesthetic and visual learning styling being most preferred while Reading/Writing was the least preferred. Furthermore, there were statistically significant differences between students learning styles based on age and gender, but it was a moderate difference. What mostly affected the preferences, however, was the type of activities or tasks, something which in turn resulted in some difference among colleges. This suggests that VARK preferences need to be related to activity type rather than be observed at individual reference. Recommendations were provided at the end of the study.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  langid = {english},
  file = {/Users/austinnicolas/Zotero/storage/NSAJX6CD/Alkooheji and Al-Hattami - 2018 - Learning Style Preferences among College Students.pdf}
}

@misc{asai2020,
  title = {Dict: {{R6 Based Key-Value Dictionary Implementation}}},
  shorttitle = {Dict},
  author = {Asai, Shun},
  year = {2020},
  month = jun,
  urldate = {2024-12-03},
  abstract = {A key-value dictionary data structure based on R6 class which is designed to be similar usages with other languages dictionary (e.g. 'Python') with reference semantics and extendabilities by R6.},
  copyright = {MIT + file LICENSE}
}

@misc{barnier2022,
  title = {Rmdformats: {{HTML Output Formats}} and {{Templates}} for 'rmarkdown' {{Documents}}},
  shorttitle = {Rmdformats},
  author = {Barnier, Julien},
  year = {2022},
  month = may,
  urldate = {2024-12-03},
  abstract = {HTML formats and templates for 'rmarkdown' documents, with some extra features such as automatic table of contents, lightboxed figures, dynamic crosstab helper.},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL ({$\geq$} 2)]}
}

@misc{chang2023,
  title = {Webshot2: {{Take Screenshots}} of {{Web Pages}}},
  shorttitle = {Webshot2},
  author = {Chang, Winston and Schloerke, Barret and Software, Posit},
  year = {2023},
  month = aug,
  urldate = {2024-12-03},
  abstract = {Takes screenshots of web pages, including Shiny applications and R Markdown documents. 'webshot2' uses headless Chrome or Chromium as the browser back-end.},
  copyright = {GPL-2}
}

@misc{garnier2024,
  title = {Viridis: {{Colorblind-Friendly Color Maps}} for {{R}}},
  shorttitle = {Viridis},
  author = {Garnier, Simon and Ross, Noam and Rudis, Bob and Sciaini, Marco and Camargo, Ant{\^o}nio Pedro and Scherer, C{\'e}dric},
  year = {2024},
  month = jan,
  urldate = {2024-12-03},
  abstract = {Color maps designed to improve graph readability for readers with common forms of color blindness and/or color vision deficiency. The color maps are also perceptually-uniform, both in regular form and also when converted to black-and-white for printing. This package also contains 'ggplot2' bindings for discrete and continuous color and fill scales. A lean version of the package called 'viridisLite' that does not include the 'ggplot2' bindings can be found at {$<$}https://cran.r-project.org/package=viridisLite{$>$}.},
  copyright = {MIT + file LICENSE},
  keywords = {Spatial}
}

@misc{gonsior2021,
  title = {{{ImitAL}}: {{Learning Active Learning Strategies}} from {{Synthetic Data}}},
  shorttitle = {{{ImitAL}}},
  author = {Gonsior, Julius and Thiele, Maik and Lehner, Wolfgang},
  year = {2021},
  month = aug,
  number = {arXiv:2108.07670},
  eprint = {2108.07670},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.07670},
  urldate = {2024-12-03},
  abstract = {One of the biggest challenges that complicates applied supervised machine learning is the need for huge amounts of labeled data. Active Learning (AL) is a well-known standard method for efficiently obtaining labeled data by first labeling the samples that contain the most information based on a query strategy. Although many methods for query strategies have been proposed in the past, no clear superior method that works well in general for all domains has been found yet. Additionally, many strategies are computationally expensive which further hinders the widespread use of AL for large-scale annotation projects. We, therefore, propose ImitAL, a novel query strategy, which encodes AL as a learning-to-rank problem. For training the underlying neural network we chose Imitation Learning. The required demonstrative expert experience for training is generated from purely synthetic data. To show the general and superior applicability of {\textbackslash}ImitAL\{\}, we perform an extensive evaluation comparing our strategy on 15 different datasets, from a wide range of domains, with 10 different state-of-the-art query strategies. We also show that our approach is more runtime performant than most other strategies, especially on very large datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/austinnicolas/Zotero/storage/LYQQPU45/Gonsior et al. - 2021 - ImitAL Learning Active Learning Strategies from Synthetic Data.pdf;/Users/austinnicolas/Zotero/storage/SRDQCCAS/2108.html}
}

@article{guan2020,
  title = {Artificial Intelligence Innovation in Education: {{A}} Twenty-Year Data-Driven Historical Analysis},
  shorttitle = {Artificial Intelligence Innovation in Education},
  author = {Guan, Chong and Mou, Jian and Jiang, Zhiying},
  year = {2020},
  month = dec,
  journal = {International Journal of Innovation Studies},
  volume = {4},
  number = {4},
  pages = {134--147},
  issn = {20962487},
  doi = {10.1016/j.ijis.2020.09.001},
  urldate = {2024-12-04},
  abstract = {Reflecting on twenty years of educational research, we retrieved over 400 research article on the application of artificial intelligence (AI) and deep learning (DL) techniques in teaching and learning. A computerised content analysis was conducted to examine how AI and DL research themes have evolved in major educational journals. By doing so, we seek to uncover the prominent keywords associated with AI-enabled pedagogical adaptation research in each decade, due to the discipline's dynamism. By examining the major research themes and historical trends from 2000 to 2019, we demonstrate that, as advanced technologies in education evolve over time, some areas of research topics seem have stood the test of time, while some others have experienced peaks and valleys. More importantly, our analysis highlights the paradigm shifts and emergent trends that are gaining prominence in the field of educational research. For instance, the results suggest the decline in conventional tech-enabled instructional design research and the flourishing of student profiling models and learning analytics. Furthermore, this paper serves to raise awareness on the opportunities and challenges behind AI and DL for pedagogical adaptation and initiate a dialogue.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/austinnicolas/Zotero/storage/TWGRZEAJ/Guan et al. - 2020 - Artificial intelligence innovation in education A twenty-year data-driven historical analysis.pdf}
}

@misc{jordon2022,
  title = {Synthetic {{Data}} -- What, Why and How?},
  author = {Jordon, James and Szpruch, Lukasz and Houssiau, Florimond and Bottarelli, Mirko and Cherubin, Giovanni and Maple, Carsten and Cohen, Samuel N. and Weller, Adrian},
  year = {2022},
  month = may,
  number = {arXiv:2205.03257},
  eprint = {2205.03257},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.03257},
  urldate = {2024-12-03},
  abstract = {This explainer document aims to provide an overview of the current state of the rapidly expanding work on synthetic data technologies, with a particular focus on privacy. The article is intended for a non-technical audience, though some formal definitions have been given to provide clarity to specialists. This article is intended to enable the reader to quickly become familiar with the notion of synthetic data, as well as understand some of the subtle intricacies that come with it. We do believe that synthetic data is a very useful tool, and our hope is that this report highlights that, while drawing attention to nuances that can easily be overlooked in its deployment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/austinnicolas/Zotero/storage/3ZNGH6C3/Jordon et al. - 2022 - Synthetic Data -- what, why and how.pdf;/Users/austinnicolas/Zotero/storage/TTZ3Z6RA/2205.html}
}

@misc{liu2024,
  title = {Can {{Medical Vision-Language Pre-training Succeed}} with {{Purely Synthetic Data}}?},
  author = {Liu, Che and Wan, Zhongwei and Wang, Haozhe and Chen, Yinda and Qaiser, Talha and Jin, Chen and Yousefi, Fariba and Burlutskiy, Nikolay and Arcucci, Rossella},
  year = {2024},
  month = oct,
  number = {arXiv:2410.13523},
  eprint = {2410.13523},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13523},
  urldate = {2024-12-03},
  abstract = {Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality image-text data, which are scarce in the medical domain. Recent advancements in Large Language Models (LLMs) and diffusion models have made it possible to generate large-scale synthetic image-text pairs. This raises the question: "Can MedVLP succeed using purely synthetic data?" To address this, we use off-the-shelf generative models to create synthetic radiology reports and paired Chest X-ray (CXR) images, and propose an automated pipeline to build a diverse, high-quality synthetic dataset, enabling a rigorous study that isolates model and training settings, focusing entirely from the data perspective. Our results show that MedVLP models trained exclusively on synthetic data outperform those trained on real data by 3.8\% in averaged AUC on zero-shot classification. Moreover, using a combination of synthetic and real data leads to a further improvement of 9.07\%. Additionally, MedVLP models trained on synthetic or mixed data consistently outperform those trained on real data in zero-shot grounding, as well as in fine-tuned classification and segmentation tasks. Our analysis suggests MedVLP trained on well-designed synthetic data can outperform models trained on real datasets, which may be limited by low-quality samples and long-tailed distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/austinnicolas/Zotero/storage/5S6XMMZT/Liu et al. - 2024 - Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data.pdf;/Users/austinnicolas/Zotero/storage/7FRAIQLJ/2410.html}
}

@article{mandinach2021,
  title = {Misconceptions about Data-Based Decision Making in Education: {{An}} Exploration of the Literature},
  shorttitle = {Misconceptions about Data-Based Decision Making in Education},
  author = {Mandinach, Ellen B. and Schildkamp, Kim},
  year = {2021},
  month = jun,
  journal = {Studies in Educational Evaluation},
  volume = {69},
  pages = {100842},
  issn = {0191491X},
  doi = {10.1016/j.stueduc.2020.100842},
  urldate = {2024-12-04},
  abstract = {Research on data-based decision making has proliferated around the world, fueled by policy recommendations and the diverse data that are now available to educators to inform their practice. Yet, many misconceptions and concerns have been raised by researchers and practitioners. To better understand the issues, a session was convened at AERA's annual convention in 2018, followed by an analysis of the literature based on misconceptions that emerged. This commentary is an outgrowth of that exploration by providing research, theoretical, and practical evidence to dispel some of the misconceptions. Our objective is to survey and synthesize the landscape of the data-based decision making literature to address the identified misconceptions and then to serve as a stimulus to changes in policy and practice as well as a roadmap for a research agenda.},
  langid = {english},
  file = {/Users/austinnicolas/Zotero/storage/QZNYNL8N/Mandinach and Schildkamp - 2021 - Misconceptions about data-based decision making in education An exploration of the literature.pdf}
}

@misc{neuwirth2022,
  title = {{{RColorBrewer}}: {{ColorBrewer Palettes}}},
  shorttitle = {{{RColorBrewer}}},
  author = {Neuwirth, Erich},
  year = {2022},
  month = apr,
  urldate = {2024-12-03},
  abstract = {Provides color schemes for maps (and other graphics) designed by Cynthia Brewer as described at http://colorbrewer2.org.},
  copyright = {Apache License 2.0},
  keywords = {Spatial}
}

@unpublished{nicolas2024,
  title = {{{PipelineEDU}}: {{Creating High-Quality Synthetic Data}} for {{Educational Research}} and {{Applications}}},
  author = {Nicolas, Austin and Sakib, Shahnewaz Karim},
  year = {2024},
  month = nov,
  abstract = {There is a scarcity of empirical education data suitable for research that does not raising ethical concerns, such as the risk of exposing sensitive student information, including personal identifiers, learning progress, and behavioral patterns. To address this, a purely synthetic dataset that can be easily generated is essential for educational research purposes. As a result, we designed a novel pipeline that generates synthetic data by establishing unique mappings between empirical data, large language models, and uniformly distributed random numbers. The resulting dataset incorporates student demographic information, learning history, potential career paths, and future areas of exploration. To safeguard privacy, foundational techniques like random shuffling and more advanced methods such as differential privacy are employed. The privacy-utility tradeoff is a key consideration in our approach, as enhancing privacy often reduces the utility of the data, and vice versa. Privacy leakage is evaluated by analyzing the accuracy of Classification models for predicting private data, while utility is measured by employing Regression and Regressification models.},
  langid = {english},
  file = {/Users/austinnicolas/Zotero/storage/7JX9L3NW/Nicolas and Sakib - PipelineEDU Creating High-Quality Synthetic Data for Educational Research and Applications.pdf}
}

@misc{ooms2024,
  title = {Jsonlite: {{A Simple}} and {{Robust JSON Parser}} and {{Generator}} for {{R}}},
  shorttitle = {Jsonlite},
  author = {Ooms, Jeroen and Lang, Duncan Temple and Hilaiel, Lloyd},
  year = {2024},
  month = sep,
  urldate = {2024-12-03},
  abstract = {A reasonably fast JSON parser and generator, optimized for statistical data and the web. Offers simple, flexible tools for working with JSON in R, and is particularly powerful for building pipelines and interacting with a web API. The implementation is based on the mapping described in the vignette (Ooms, 2014). In addition to converting JSON data from/to R objects, 'jsonlite' contains functions to stream, validate, and prettify JSON data. The unit tests included with the package verify that all edge cases are encoded and decoded consistently for use with dynamic data in systems and applications.},
  copyright = {MIT + file LICENSE},
  keywords = {WebTechnologies}
}

@misc{rudis2024,
  title = {Hrbrthemes: {{Additional Themes}}, {{Theme Components}} and {{Utilities}} for 'Ggplot2'},
  shorttitle = {Hrbrthemes},
  author = {Rudis, Bob},
  year = {2024},
  month = mar,
  urldate = {2024-12-03},
  abstract = {A compilation of extra 'ggplot2' themes, scales and utilities, including a spell check function for plot label fields and an overall emphasis on typography. A copy of the 'Google' font 'Roboto Condensed' is also included.},
  collaborator = {Kennedy, Patrick and Reiner, Philipp and Wilson, Dan and Adam, Xavier and Barnett, Jacob and Leeper, Thomas J. and Meys, Joris},
  copyright = {MIT + file LICENSE}
}

@article{shao2021,
  title = {Degree {{Planning}} with {{PLAN-BERT}}: {{Multi-Semester Recommendation Using Future Courses}} of {{Interest}}},
  shorttitle = {Degree {{Planning}} with {{PLAN-BERT}}},
  author = {Shao, Erzhuo and Guo, Shiyuan and Pardos, Zachary A.},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {17},
  pages = {14920--14929},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v35i17.17751},
  urldate = {2024-12-04},
  abstract = {Planning scenarios involving user pre-specified items present themselves frequently in recommender system domains. Although next-item and next-basket recommendation has been a focus of prior research, multiple consecutive item or basket approaches are needed for planning. No prior work has leveraged pre-specified future reference items to improve this type of challenging consecutive prediction task at inference time. PLAN-BERT is the first to accommodate this general planning scenario. It does so by contributing novel modifications that take inspiration from the masked training and contextual embedding of self-attention models. To test the model, we use the domain of student academic degree planning, in which students' past course histories and future pre-specified courses of interest are used to fill in the remainder of their curriculum. Our offline analyses consist of 15 million historic course enrollments at 20 institutions and an online evaluation conducted at one of the institutions. Our results show that PLAN-BERT outperforms existing models including BERT, BiLSTM, and a UserKNN baseline, with small numbers of future reference items substantially improving accuracy. Significant results from our online evaluation show PLAN-BERT to be strongest in students' perceptions of personalization.},
  langid = {english},
  file = {/Users/austinnicolas/Zotero/storage/YLQXQYC6/Shao et al. - 2021 - Degree Planning with PLAN-BERT Multi-Semester Recommendation Using Future Courses of Interest.pdf}
}

@misc{sievert2024,
  title = {Plotly: {{Create Interactive Web Graphics}} via 'Plotly.Js'},
  shorttitle = {Plotly},
  author = {Sievert, Carson and Parmer, Chris and Hocking, Toby and Chamberlain, Scott and Ram, Karthik and Corvellec, Marianne and Despouy, Pedro and Br{\"u}ggemann, Salim and Inc, Plotly Technologies},
  year = {2024},
  month = jan,
  urldate = {2024-12-03},
  abstract = {Create interactive web graphics from 'ggplot2' graphs and/or a custom interface to the (MIT-licensed) JavaScript library 'plotly.js' inspired by the grammar of graphics.},
  copyright = {MIT + file LICENSE},
  keywords = {DynamicVisualizations,WebTechnologies}
}

@inproceedings{wang2021,
  title = {Real-{{ESRGAN}}: {{Training Real-World Blind Super-Resolution}} with {{Pure Synthetic Data}}},
  shorttitle = {Real-{{ESRGAN}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Wang, Xintao and Xie, Liangbin and Dong, Chao and Shan, Ying},
  year = {2021},
  month = oct,
  pages = {1905--1914},
  publisher = {IEEE},
  address = {Montreal, BC, Canada},
  doi = {10.1109/ICCVW54120.2021.00217},
  urldate = {2024-12-03},
  abstract = {Though many attempts have been made in blind superresolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex realworld degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-66540-191-3},
  langid = {english},
  file = {/Users/austinnicolas/Zotero/storage/UD8R7TPX/Wang et al. - 2021 - Real-ESRGAN Training Real-World Blind Super-Resolution with Pure Synthetic Data.pdf}
}

@misc{wickham2023,
  title = {Tidyverse: {{Easily Install}} and {{Load}} the '{{Tidyverse}}'},
  shorttitle = {Tidyverse},
  author = {Wickham, Hadley},
  year = {2023},
  month = feb,
  urldate = {2024-12-03},
  abstract = {The 'tidyverse' is a set of packages that work in harmony because they share common data representations and 'API' design. This package is designed to make it easy to install and load multiple 'tidyverse' packages in a single step. Learn more about the 'tidyverse' at {$<$}https://www.tidyverse.org{$>$}.},
  copyright = {MIT + file LICENSE},
  keywords = {ChemPhys}
}

@misc{xie2024,
  title = {Knitr: {{A General-Purpose Package}} for {{Dynamic Report Generation}} in {{R}}},
  shorttitle = {Knitr},
  author = {Xie, Yihui and Sarma, Abhraneel and Vogt, Adam and Andrew, Alastair and Zvoleff, Alex and {Al-Zubaidi}, Amar and package {http://www.andre-simon.de)}, Andre Simon (the CSS files under inst/themes/ were derived from the Highlight and Atkins, Aron and Wolen, Aaron and Manton, Ashley and Yasumoto, Atsushi and Baumer, Ben and Diggs, Brian and Zhang, Brian and Yapparov, Bulat and Pereira, Cassio and Dervieux, Christophe and Hall, David and {Hugh-Jones}, David and Robinson, David and Hemken, Doug and Murdoch, Duncan and Campitelli, Elio and Hughes, Ellis and Riederer, Emily and Hirschmann, Fabian and Simeon, Fitch and Fang, Forest and Harrell, Frank E. and {Aden-Buie}, Garrick and Detrez, Gregoire and Wickham, Hadley and Zhu, Hao and Jeon, Heewon and Bengtsson, Henrik and Yutani, Hiroaki and Lyttle, Ian and Daniel, Hodges and Bien, Jacob and Burkhead, Jake and Manton, James and Lander, Jared and Punyon, Jason and Luraschi, Javier and Arnold, Jeff and Bryan, Jenny and Ashkenas, Jeremy and Stephens, Jeremy and Hester, Jim and Cheng, Joe and Ranke, Johannes and Honaker, John and Muschelli, John and Keane, Jonathan and Allaire, J. J. and Toloe, Johan and Sidi, Jonathan and Larmarange, Joseph and Barnier, Julien and Zhong, Kaiyin and Slowikowski, Kamil and Forner, Karl and Smith, Kevin K. and Mueller, Kirill and Takahashi, Kohske and Walthert, Lorenz and Gallindo, Lucas and Hofert, Marius and Modr{\'a}k, Martin and Chirico, Michael and Friendly, Michael and Bojanowski, Michal and Kuhlmann, Michel and Patrick, Miller and Caballero, Nacho and Salkowski, Nick and Hansen, Niels Richard and Ross, Noam and Mahdi, Obada and Krivitsky, Pavel N. and Faria, Pedro and Li, Qiang and Vaidyanathan, Ramnath and Cotton, Richard and Krzyzanowski, Robert and Copetti, Rodrigo and Francois, Romain and Williamson, Ruaridh and Mati, Sagiru and Kostyshak, Scott and Meyer, Sebastian and Brouwer, Sietse and de Bernard, Simon and Rousseau, Sylvain and Wei, Taiyun and Assus, Thibaut and Lamadon, Thibaut and Leeper, Thomas and Mastny, Tim and {Torsney-Weir}, Tom and Davis, Trevor and Veitas, Viktoras and Zhu, Weicheng and Wu, Wush and Foster, Zachary and Kamvar, Zhian N.},
  year = {2024},
  month = nov,
  urldate = {2024-12-03},
  abstract = {Provides a general-purpose tool for dynamic report generation in R using Literate Programming techniques.},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL]},
  keywords = {ReproducibleResearch}
}

@misc{zotero-106,
  title = {College {{Majors Explorer}}},
  journal = {Big Economics},
  urldate = {2024-12-03},
  abstract = {Back to College Dashboard The College Majors Explorer is a tool that allows you to compare college majors quickly on key outcomes. These include: Starting Income, Lifetime Income, Difficulty of the major, Early career unemployment rate of the major, Field of the major, The share of graduates from the major that are female, The share of graduates in each major that work in its top 10 occupations},
  howpublished = {https://bigeconomics.org/college-majors-explorer/},
  langid = {american},
  file = {/Users/austinnicolas/Zotero/storage/TF7UVZXA/college-majors-explorer.html}
}
